{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global inputs\n",
    "timestamps = 'TIMESTAMP(\"2023-09-25\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set the credentials\n",
    "key_path_home_dir = os.path.expanduser(\"~\") + \"/bq_credentials.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path_home_dir, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# Now, instantiate the client and upload the table to BigQuery\n",
    "client = bigquery.Client(project=\"web-scraping-371310\", credentials=credentials)\n",
    "\n",
    "# Query the NULL records in the \"lukas_mobile_de\" table\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `web-scraping-371310.crawled_datasets.lukas_mobile_de`\n",
    "    WHERE TIMESTAMP_TRUNC(crawled_timestamp, DAY) IN ({timestamps}) AND titel IS NULL\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "df_null_records = pd.DataFrame(client.query(query=query).result().to_dataframe())\n",
    "\n",
    "# Sort the dataframe by the crawled_timestamp, marke, modell\n",
    "df_null_records = df_null_records.sort_values(\n",
    "    by=[\"crawled_timestamp\", \"marke\", \"modell\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the URLs to be crawled in the correct JSON format\n",
    "final_urls = []\n",
    "for m in df_null_records[\"marke\"].unique():\n",
    "    df_iter = df_null_records[df_null_records[\"marke\"] == m].reset_index(drop=True)\n",
    "    urls_iter = []\n",
    "    for i in range(len(df_iter)):\n",
    "        output_dict = {\n",
    "            \"marke\": df_iter[\"marke\"][i],\n",
    "            \"modell\": df_iter[\"modell\"][i],\n",
    "            \"last_page\": int(df_iter[\"total_num_pages\"][i]),\n",
    "            \"page_rank\": int(df_iter[\"page_rank\"][i]),\n",
    "            \"car_page_url\": df_iter[\"url_to_crawl\"][i],\n",
    "        }\n",
    "        urls_iter.append(output_dict)\n",
    "    final_urls.append(urls_iter)\n",
    "    \n",
    "# Write the results to a JSON file\n",
    "with open(f\"car_page_url_list_cat_all.json\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(obj=final_urls, fp=f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
